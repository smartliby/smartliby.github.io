---
title: kubeflow1.0 安装教程
date: 2020-04-04 18:21:11
categories:
- kubeflow
tags:
- k8s
- kubeflow
- microk8s
---

[Kubeflow](https://www.kubeflow.org/docs)是Google推出的基于kubernetes环境下的机器学习组件，通过Kubeflow可以实现对TFJob等资源类型定义，可以像部署应用一样完成在TFJob分布式训练模型的过程。最初的设计是将Kubernetes和Tensorflow结合实现对Tensorflow分布式训练的支持。但是仅仅实现对Tensorflow的支持还是远远不够的，`Kubeflow`社区又陆续对各种深度学习框架进行支持,例如:`MXNet`, `Caffee`, `PyTorch`等。使得机器学习算法同学只需关心算法实现，而后续的模型训练和服务上线都交给平台来做,解放算法同学使其专做自己擅长的事儿。

<!-- more -->

目前Kubeflow仅支持在v1.15.11及以下版本的k8s上部署，v1.16及以上存在兼容性问题，具体情况可参考[这篇文章](https://www.kubeflow.org/docs/started/k8s/overview/)，接下来我们基于[Microk8s](https://microk8s.io/)（版本1.15/stable）部署kubeflow v1.0

#### 安装 knative v0.7.1

kubeflow安装时依赖knative和istio组件，所以需要先安装knative，istio(microk8s安装时已安装`microk8s.enable istio`，不需要再安装)，knative 的镜像是用 sha256 方式来 pull的。在本地我们不能通过 docker tag 的方式重新打包，只能修改 yaml 里的所有配置

默认knative的安装配置文件在`/snap/microk8s/current/actions/knative/`目录下，没有操作权限无法修改，需要先拷贝出来或者去github下载到本地，并将里面的镜像下载域名由gcr.io改成gcr.azk8s.cn

```shell
# 安装 Knative CRD
kubectl apply --selector knative.dev/crd-install=true -f serving.yaml

# 再运行一遍 kubectl apply
kubectl apply  -f serving.yaml
```

#### 验证 knative 安装

运行`kubectl get pods --namespace knative-serving`验证 Knative 所有 pod 是否可以正常启动

```shell
NAME                                      READY   STATUS    RESTARTS   AGE
activator-55f6c8d9b-2hs77                 2/2     Running   24         2d13h
autoscaler-78d575f875-l7zfj               2/2     Running   18         2d13h
controller-776478fb94-hmmxc               1/1     Running   3          2d13h
networking-certmanager-779cd6f448-xtvrb   1/1     Running   3          2d13h
networking-istio-674bd78b75-688gc         1/1     Running   3          2d13h
webhook-59b575dc79-cs7f4                  1/1     Running   0          13h
```

#### 安装kubeflow

下载 kfctl binary from the [Kubeflow releases page](https://github.com/kubeflow/kfctl/releases/)

解压安装包并添加到执行目录

```
tar -xvf kfctl_v1.0.1-0-gf3edb9b_linux.tar.gz
sudo cp kfctl /usr/bin
```

设置环境

```
export BASE_DIR=/data/
export KF_NAME=my-kubeflow
export KF_DIR=${BASE_DIR}/${KF_NAME}
export CONFIG_URI="https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_k8s_istio.v1.0.1.yaml"
```

部署kubeflow

```
mkdir -p ${KF_DIR}
cd ${KF_DIR}
kfctl apply -V -f ${CONFIG_URI}
kubectl -n kubeflow get all
```

输出如下日志即安装成功

```
INFO[0259] Applied the configuration Successfully!       filename="cmd/apply.go:72"
```

查看当前 Kubernetes pods 状态

```
kubectl get pods --namespace kubeflow
```

发现kubeflow的pod大部分没有启动成功，原因还是网络问题，需要访问gcr.io下载镜像

执行以下脚本将镜像下载到本地并导入到microk8s

```shell
#!/usr/bin/env bash

echo ""
echo "=========================================================="
echo "pull kubeflow  v1.0 images from gcr.azk8s.cn ..."
echo "=========================================================="
echo ""

ORIGINAL_REGISTRY=gcr.io
MY_REGISTRY=gcr.azk8s.cn

gcr_imgs=(
    "kfserving/kfserving-controller:0.2.2"
    "ml-pipeline/api-server:0.2.0"
    "kubeflow-images-public/kfam:v1.0.0-gf3e09203"
    "kubeflow-images-public/ingress-setup:latest"
    "kubeflow-images-public/kubernetes-sigs/application:1.0-beta"
    "kubeflow-images-public/centraldashboard:v1.0.0-g3ec0de71"
    "kubeflow-images-public/jupyter-web-app:v1.0.0-g2bd63238"
    "kubeflow-images-public/katib/v1alpha3/katib-controller:v0.8.0"
    "kubeflow-images-public/katib/v1alpha3/katib-db-manager:v0.8.0"
    "kubeflow-images-public/katib/v1alpha3/katib-ui:v0.8.0"
    "kubebuilder/kube-rbac-proxy:v0.4.0"
    "metacontroller/metacontroller:v0.3.0"
    "kubeflow-images-public/metadata:v0.1.11"
    "ml-pipeline/envoy:metadata-grpc"
    "tfx-oss-public/ml_metadata_store_server:v0.21.1"
    "kubeflow-images-public/metadata-frontend:v0.1.8"
    "ml-pipeline/visualization-server:0.2.0"
    "ml-pipeline/persistenceagent:0.2.0"
    "ml-pipeline/scheduledworkflow:0.2.0"
    "ml-pipeline/frontend:0.2.0"
    "ml-pipeline/viewer-crd-controller:0.2.0"
    "kubeflow-images-public/notebook-controller:v1.0.0-gcd65ce25"
    "kubeflow-images-public/profile-controller:v1.0.0-ge50a8531"
    "kubeflow-images-public/pytorch-operator:v1.0.0-g047cf0f"
    "spark-operator/spark-operator:v1beta2-1.0.0-2.4.4"
    "spark-operator/spark-operator:v1beta2-1.0.0-2.4.4"
    "google_containers/spartakus-amd64:v1.1.0"
    "kubeflow-images-public/tf_operator:v1.0.0-g92389064"
    "kubeflow-images-public/admission-webhook:v1.0.0-gaf96e4e3"
)

for img in ${gcr_imgs[@]}
do
    # 拉取镜像
    docker pull ${MY_REGISTRY}/${img}
    # 添加Tag
    docker tag ${MY_REGISTRY}/${img} ${ORIGINAL_REGISTRY}/${img}
    # 输出
    docker save ${ORIGINAL_REGISTRY}/${img} > ${img##*/}.tar
    # 输入
    microk8s.ctr -n k8s.io image import ${img##*/}.tar
    # 删除Tag
    docker rmi ${MY_REGISTRY}/${img} ${ORIGINAL_REGISTRY}/${img}
done

echo ""
echo "=========================================================="
echo "pull kubeflow  v1.0 images from gcr.azk8s.cn finished."
echo "=========================================================="
echo ""
```

如果还是有pod无法启动，可通过`kubectl describe pod 未启动pod的名称 -n kubeflow`查看原因

如果是因为镜像无法下载，可以将依赖的镜像加到上面的脚本里下载镜像

如果是因为镜像拉取策略导致每次都重新下载问题，可通过下面命令或者kubernetes-dashboard修改，将`Always` 改为 `IfNotPresent`

```
kubectl edit pod 未启动pod的名称 -n kubeflow
```

执行 `kubectl get pods --namespace kubeflow`查看kubeflow的pod都已运行起来

```
NAME                                                           READY   STATUS      RESTARTS   AGE
admission-webhook-bootstrap-stateful-set-0                     1/1     Running     6          2d4h
admission-webhook-deployment-569558c8b6-n8b7k                  1/1     Running     0          13h
application-controller-stateful-set-0                          1/1     Running     3          2d4h
argo-ui-7ffb9b6577-w8pb7                                       1/1     Running     7          3d7h
centraldashboard-659bd78c-fxgqd                                1/1     Running     3          3d7h
jupyter-web-app-deployment-878f9c988-xgh82                     1/1     Running     3          2d5h
katib-controller-7f58569f7d-8bw7z                              1/1     Running     4          3d7h
katib-db-manager-54b66f9f9d-ngqw9                              1/1     Running     3          3d7h
katib-mysql-dcf7dcbd5-7wbck                                    1/1     Running     12         4d1h
katib-ui-6f97756598-4mtjs                                      1/1     Running     3          3d7h
kfserving-controller-manager-0                                 2/2     Running     7          2d5h
metacontroller-0                                               1/1     Running     5          3d7h
metadata-db-65fb5b695d-wq8vh                                   1/1     Running     12         4d1h
metadata-deployment-65ccddfd4c-vwfd2                           1/1     Running     3          3d7h
metadata-envoy-deployment-7754f56bff-svtz2                     1/1     Running     3          3d7h
metadata-grpc-deployment-75f9888cbf-zj4sn                      1/1     Running     5          3d7h
metadata-ui-7c85545947-v68l7                                   1/1     Running     3          3d7h
minio-69b4676bb7-w96xk                                         1/1     Running     12         4d1h
ml-pipeline-5cddb75848-bsc48                                   1/1     Running     3          2d6h
ml-pipeline-ml-pipeline-visualizationserver-7f6fcb68c8-vxjj7   1/1     Running     3          2d7h
ml-pipeline-persistenceagent-6ff9fb86dc-dvxx4                  1/1     Running     5          2d6h
ml-pipeline-scheduledworkflow-7f84b54646-ndxcb                 1/1     Running     3          2d7h
ml-pipeline-ui-6758f58868-gqvlp                                1/1     Running     3          2d6h
ml-pipeline-viewer-controller-deployment-685874bc58-jljw8      1/1     Running     3          2d5h
mysql-6bcbfbb6b8-xmphz                                         1/1     Running     12         4d1h
notebook-controller-deployment-7db7c8589d-mlgb4                1/1     Running     3          2d5h
profiles-deployment-56b7c6788f-kk8kh                           2/2     Running     6          2d7h
pytorch-operator-cf8c5c497-nmfnv                               1/1     Running     7          3d7h
seldon-controller-manager-6b4b969447-qp7l4                     1/1     Running     20         4d1h
spark-operatorcrd-cleanup-rrpxd                                0/2     Completed   0          3d7h
spark-operatorsparkoperator-76dd5f5688-kn28n                   1/1     Running     3          3d7h
spartakus-volunteer-5dc96f4447-xjclm                           1/1     Running     3          3d7h
tensorboard-5f685f9d79-9x549                                   1/1     Running     12         4d1h
tf-job-operator-5fb85c5fb7-lqvrg                               1/1     Running     6          3d7h
workflow-controller-689d6c8846-znvt9                           1/1     Running     12         4d1h
```

执行如下命令进行端口映射访问Kubeflow UI

```
nohup kubectl port-forward -n istio-system svc/istio-ingressgateway 8088:80 &
```

访问http://127.0.0.1:8088/

![](/images/media/选区_053.png)

#### 创建Jupyter notebook server

![](/images/media/选区_054.png)

![](/images/media/选区_055.png)

![](/images/media/选区_056.png)

点击连接之后就可以跑模型训练了

#### 测试Jupyter

创建Python 3 notebook，执行如下代码

```
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

import tensorflow as tf

x = tf.placeholder(tf.float32, [None, 784])

W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))

y = tf.nn.softmax(tf.matmul(x, W) + b)

y_ = tf.placeholder(tf.float32, [None, 10])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)

sess = tf.InteractiveSession()
tf.global_variables_initializer().run()

for _ in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print("Accuracy: ", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
```

运行结果如下

```
Accuracy:  0.9012
```

